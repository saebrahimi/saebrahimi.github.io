---
---

@string{aaai = {Association for the Advancement of Artificial Intelligence}}
@string{corl = {Conference on Robot Learning}}
@string{iccv = {International Conference on Computer Vision}}
@string{iclr = {International Conference on Learning Representations}}
@string{icmi = {International Conference on Multimodal Interaction}}
@string{icml = {International Conference on Machine Learning}}
@string{neurips = {Advances in Neural Information Processing Systems}}
@string{nips = {Advances in Neural Information Processing Systems}}
@string{wacv = {Winter Conference on Applications of Computer Vision}}

@article{kanaa2019simple,
  title={Simple Video Generation using Neural ODEs},
  author={Kanaa, David and Voleti, Vikram and Kahou, Samira and Pal, Christopher},
  journal={NeurIPS workshop LIRE},
  year={2019}
}

@article{birhane2019highres,
  title={HighRes-net: Multi-Frame Super-Resolution by Recursive Fusion},
  author={Birhane, Israel G. and Kalaitzis, Alfredo and Sankaran, Kris and Cornebise, Julien and Bengio, Yoshua and Deudon, Michel and Lin, Zhichao and Michalski, Vincent and Ebrahimi Kahou, Samira and Arefin, Rifat},
  journal={NeurIPS workshop SEDL},
  year={2019},
  code={https://github.com/ElementAI/HighRes-net}
}

@article{weiss2019navigation,
  title={Navigation Agents for the Visually Impaired: A Sidewalk Simulator and Experiments},
  author={Weiss, Martin and Chamorro, Simon and Girgis, Roger and Luck, Margaux and Kahou, Samira E and Cohen, Joseph P and Nowrouzezahrai, Derek and Precup, Doina and Golemo, Florian and Pal, Chris},
  journal=corl,
  year={2019},
  abbr={CoRL},
  projectpage={https://mweiss17.github.io/SEVN/}
}

@inproceedings{el2019tell,
  title={Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction},
  author={El-Nouby, Alaaeldin and Sharma, Shikhar and Schulz, Hannes and Hjelm, Devon and Asri, Layla El and Kahou, Samira Ebrahimi and Bengio, Yoshua and Taylor, Graham W},
  booktitle=iccv,
  pages={10304--10312},
  year={2019},
  abbr={ICCV19}
}

@inproceedings{fatemi2019dead,
  title={Dead-ends and Secure Exploration in Reinforcement Learning},
  author={Fatemi, Mehdi and Sharma, Shikhar and Van Seijen, Harm and Kahou, Samira Ebrahimi},
  booktitle=icml,
  pages={1873--1881},
  year={2019},
  abbr={ICML}
}

@article{michalski2019empirical,
  title={An Empirical Study of Batch Normalization and Group Normalization in Conditional Computation},
  author={Michalski, Vincent and Voleti, Vikram and Kahou, Samira Ebrahimi and Ortiz, Anthony and Vincent, Pascal and Pal, Chris and Precup, Doina},
  journal={ICML workshop Understanding and Improving Generalization in Deep Learning},
  year={2019}
}

@article{chandar2019towards,
  title={Towards Non-saturating Recurrent Units for Modelling Long-term Dependencies},
  author={Chandar*, Sarath and Sankar*, Chinnadhurai and Vorontsov, Eugene and Kahou, Samira Ebrahimi and Bengio, Yoshua},
  journal=aaai,
  year={2019},
  abbr={AAAI}
}

@inproceedings{kim2019deep,
  title={Deep-hurricane-tracker: Tracking and forecasting extreme climate events},
  author={Kim, Sookyung and Kim, Hyojin and Lee, Joonseok and Yoon, Sangwoong and Kahou, Samira Ebrahimi and Kashinath, Karthik and Prabhat, Mr},
  booktitle=wacv,
  pages={1761--1769},
  year={2019},
  organization={IEEE},
  abbr={WACV},
  note={earlier version was presented at Climate Informatics 2018 (spotlight oral)}
}

@inproceedings{li2018towards,
  title={Towards deep conversational recommendations},
  author={Li, Raymond and Kahou, Samira Ebrahimi and Schulz, Hannes and Michalski, Vincent and Charlin, Laurent and Pal, Chris},
  booktitle=neurips,
  pages={9725--9735},
  year={2018},
  abbr={NeurIPS},
  projectpage={https://redialdata.github.io/website/}
}

@article{sharma2018chatpainter,
  title={Chatpainter: Improving text to image generation using dialogue},
  author={Sharma, Shikhar and Suhubdy, Dendi and Michalski, Vincent and Kahou, Samira Ebrahimi and Bengio, Yoshua},
  journal={ICLR workshop},
  year={2018}
}

@article{kahou2017figureqa,
  title={Figureqa: An annotated figure dataset for visual reasoning},
  author={Kahou*, Samira Ebrahimi and Michalski*, Vincent and Atkinson, Adam and K{\'a}d{\'a}r, {\'A}kos and Trischler, Adam and Bengio, Yoshua},
  journal={ICLR workshop},
  year={2018},
  code={https://github.com/vmichals/FigureQA-baseline},
  projectpage={https://www.microsoft.com/en-us/research/publication/figureqa-an-annotated-figure-dataset-for-visual-reasoning/}
}

@inproceedings{racah2017extremeweather,
  title={Extremeweather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events},
  author={Racah, Evan and Beckham, Christopher and Maharaj, Tegan and Kahou, Samira Ebrahimi and Prabhat, Mr and Pal, Chris},
  booktitle=nips,
  pages={3402--3413},
  year={2017},
  abbr={NeurIPS},
  projectpage={https://extremeweatherdataset.github.io/}
}

@article{kim2017segmenting,
    title={Segmenting and tracking extreme climate events using neural networks},
    author={Kim, Sookyung and Mudigonda, Mayur and Mahesh, Ankur and Kahou, Samira and Kashinath, Karthik and Williams, Dean and Michalski, Vincent and O'Brien, Travis and Prabhat, Mr},
    journal={NeurIPS workshop Deep Learning for Physical Science},
    year={2017}
}

@inproceedings{goyal2017something,
  title={The "Something Something" Video Database for Learning and Evaluating Visual Common Sense},
  author={Goyal, Raghav and Kahou, Samira Ebrahimi and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle=iccv,
  volume={1},
  number={2},
  pages={3},
  year={2017},
  abbr={ICCV17},
  projectpage={https://20bn.com/datasets/something-something/v1}
}

@inproceedings{ebrahimi2017ratm,
  title={RATM: Recurrent attentive tracking model},
  author={Ebrahimi Kahou, Samira and Michalski, Vincent and Memisevic, Roland and Pal, Christopher and Vincent, Pascal},
  booktitle={CVPR workshop Brave New Ideas in Motion Representations},
  pages={10--19},
  year={2017},
  code={https://github.com/saebrahimi/RATM},
  note={oral presentation}
}

@article{urban2016deep,
  title={Do deep convolutional nets really need to be deep and convolutional?},
  author={Urban, Gregor and Geras, Krzysztof J and Kahou, Samira Ebrahimi and Aslan, Ozlem and Wang, Shengjie and Caruana, Rich and Mohamed, Abdelrahman and Philipose, Matthai and Richardson, Matt},
  journal=iclr,
  year={2017},
  abbr={ICLR}
}

@thesis{ebrahimi2016emotion,
  title={Emotion Recognition with Deep Neural Networks},
  author={Ebrahimi Kahou, Samira},
  year={2016},
  school={{\'E}cole Polytechnique de Montr{\'e}al, Best Thesis Award in the Department of Computer Engineering},
  note={Best Thesis Award in the Department of Computer Engineering},
  remotepdf={https://publications.polymtl.ca/2290/1/2016_SamiraEbrahimiKahou.pdf},
  remoteslides={https://drive.google.com/open?id=1Of0h0T_g_Wh7AEO_LLUKSVD5hBNc2OJSlJJUafwXN_U}
}

@article{team2016theano,
  title={Theano: A Python framework for fast computation of mathematical expressions},
  author={{The Theano Development Team}},
  journal={Technical Report},
  year={2016},
  arxiv={1605.02688}
}

@article{kahou2016emonets,
  title={Emonets: Multimodal deep learning approaches for emotion recognition in video},
  author={Kahou, Samira Ebrahimi and Bouthillier, Xavier and Lamblin, Pascal and Gulcehre, Caglar and Michalski, Vincent and Konda, Kishore and Jean, S{\'e}bastien and Froumenty, Pierre and Dauphin, Yann and Boulanger-Lewandowski, Nicolas and others},
  journal={Journal on Multimodal User Interfaces},
  volume={10},
  number={2},
  pages={99--111},
  year={2016},
  publisher={Springer},
  abbr={JMUI},
  arxiv={1503.01800}
}

@inproceedings{ebrahimi2015recurrent,
  title={Recurrent neural networks for emotion recognition in video},
  author={Ebrahimi Kahou, Samira and Michalski, Vincent and Konda, Kishore and Memisevic, Roland and Pal, Christopher},
  booktitle=icmi,
  pages={467--474},
  year={2015},
  organization={ACM},
  abbr={ICMI},
  code={https://github.com/saebrahimi/Emotion-Recognition-RNN},
  note={Entry to the ICMI 2015 Grand Challenge on Emotion Recognition in the Wild (2nd runner up)},
  pdf={papers/emotion_rnns.pdf}
}

@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal=iclr,
  year={2015},
  abbr={ICLR}
}

@inproceedings{kahou2014facial,
  title={Facial expression analysis based on high dimensional binary features},
  author={Kahou, Samira Ebrahimi and Froumenty, Pierre and Pal, Christopher},
  booktitle={ECCV workshop on computer vision with local binary patterns},
  pages={135--147},
  year={2014},
  organization={Springer},
  note={Best Paper Award},
  pdf={papers/lbp.pdf}
}

@inproceedings{kahou2013combining,
  title={Combining modality specific deep neural networks for emotion recognition in video},
  author={Kahou, Samira Ebrahimi and Pal, Christopher and Bouthillier, Xavier and Froumenty, Pierre and G{\"u}l{\c{c}}ehre, {{\c{C}}aglar, *} and Memisevic, Roland and Vincent, Pascal and Courville, Aaron and Bengio, Yoshua},
  booktitle=icmi,
  pages={543--550},
  year={2013},
  organization={ACM},
  abbr={ICMI},
  note={* please see the additional authors section for complete author list. Winning entry to the ICMI 2013 Grand Challenge on Emotion Recognition in the Wild},
  pdf={papers/icmi_emotiw.pdf}
}

@inproceedings{sulema2010image,
  title={Image compression: Comparative analysis of basic algorithms},
  author={Sulema, Yevgeniya and Kahou, Samira Ebrahimi},
  booktitle={East-West Design \& Test Symposium (EWDTS)},
  pages={534--537},
  year={2010},
  organization={IEEE}
}

@inproceedings{kahou2010statistical,
  title={Statistical approach to image classification},
  author={Kahou, Samira Ebrahimi and Sulema, Yevgeniya},
  booktitle={IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)},
  volume={3},
  pages={1--4},
  year={2010},
  organization={IEEE}
}
